{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27131623",
   "metadata": {},
   "source": [
    "# 모델 학습과 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fb3a4",
   "metadata": {},
   "source": [
    "## 모델의 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347c0a8",
   "metadata": {},
   "source": [
    "### 지도 학습 vs 비지도 학습\n",
    "\n",
    "- 지도 학습 (Supervised Learning)\n",
    "  - 입력에 대한 정답 (Label, Ground Truth)이 존재  \n",
    "  - [입력 - 정답] 관계를 학습하여 새로운 입력에 대해 정답을 맞추는 과정\n",
    "\n",
    "- 비지도 학습 (Unsupervised Learning)\n",
    "  - 정답이 없음\n",
    "  - 데이터로부터 어떤 알고리즘을 통해 유용한 정보를 추출  \n",
    "\n",
    "![](https://www.researchgate.net/publication/329533120/figure/fig1/AS:702267594399761@1544445050584/Supervised-learning-and-unsupervised-learning-Supervised-learning-uses-annotation.png)\n",
    "<br /><sub>출처: https://www.researchgate.net/figure/Supervised-learning-and-unsupervised-learning-Supervised-learning-uses-annotation_fig1_329533120</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda1671",
   "metadata": {},
   "source": [
    "\n",
    "## 학습 매개변수(Trainable Parameter)\n",
    "- 학습 시, 값이 변화하는 매개변수  \n",
    "  이 매개변수에 따라 학습 알고리즘(모델)이 변함\n",
    "\n",
    "- 입력에 따른 출력을 나타내는 수식, 이를 학습 모델이라고 칭함  \n",
    "  선형회귀(Linear Regression)을 예로 들면,\n",
    "\n",
    "### $\\qquad \\quad Y = aX + b $  \n",
    "\n",
    "  - $X$ : 입력\n",
    "  - $Y$ : 출력\n",
    "  - $a, b$ : 학습 매개변수\n",
    "\n",
    "- 초기화된 모델(Left Image)로부터 학습이 진행되면서  \n",
    "  학습 데이터에 맞는 모델(Right Image)로 학습 파라미터(Trainable Parameters)를 수정해 나가는 과정 \n",
    "  \n",
    "![](https://learningstatisticswithr.com/book/lsr_files/figure-html/regression1b-1.png)\n",
    "\n",
    "![](https://learningstatisticswithr.com/book/lsr_files/figure-html/regression1a-1.png)\n",
    "\n",
    "<sub>출처: https://learningstatisticswithr.com/book/regression.html</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7d67d",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터(Hyper Parameter)\n",
    "- 사람이 직접 설정해야하는 매개변수\n",
    "- 학습이 되기전 미리 설정되어 상수취급\n",
    "  - 손실 함수 (Cost Function)\n",
    "  - 학습률 (Learning Rate)\n",
    "  - 학습 반복 횟수 (Epochs)\n",
    "  - 미니 배치 크기 (Batch Size)\n",
    "  - 은닉층의 노드 개수 (Units)\n",
    "  - 노이즈 (Noise)\n",
    "  - 규제화 (Regularization)\n",
    "  - 가중치 초기화 (Weights Initialization)\n",
    "\n",
    "- 신경망의 매개변수인 가중치는 학습 알고리즘에 의해 **자동**으로 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c6b32",
   "metadata": {},
   "source": [
    "## 손실함수 (Loss Function, Cost Function)\n",
    "\n",
    "- 학습이 진행되면서 해당 과정이 얼마나 잘 되고 있는지 나타내는 지표\n",
    "- 손실 함수에 따른 결과를 통해 학습 파라미터를 조정\n",
    "- 최적화 이론에서 최소화 하고자 하는 함수\n",
    "- <u>**미분 가능한 함수 사용**</u>\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SMxO-SrTzm8-Fq07T_rdkLnukxgPlmuL)\n",
    "<br /><sub>출처: https://zhuanlan.zhihu.com/p/85540935\\</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a6c52",
   "metadata": {},
   "source": [
    "### **학습**의 수학적 의미  \n",
    "\n",
    "![](https://s3.amazonaws.com/images.internalpointers.com/2017/10/non-convex-function.svg)\n",
    "<br /><sub>출처: https://www.internalpointers.com/post/cost-function-logistic-regression</sub>\n",
    "\n",
    "  ### $\\qquad \\qquad \\tilde{\\theta} = arg \\underset{\\theta} min \\ L(x, y\\ ; \\theta) $\n",
    "\n",
    "  - L : 손실 함수\n",
    "  - x : 학습에 사용되는 데이터의 입력값\n",
    "  - y : 학습에 사용되는 데이터의 정답\n",
    "  - $\\theta$ : 학습될 모든 파라미터(parameter)를 모은 벡터\n",
    "  - $\\tilde{\\theta}$ : 추정된 최적의 파라미터\n",
    "\n",
    "  - 학습에 사용되는 파라미터(parameter, $\\ i.g \\ $가중치($weights$), 편향($bias$), $\\ ...$ )를 모두 통칭해서 $\\theta$로 표현 가능  \n",
    "  이러한 $\\theta$의 최적값을 찾는 것이 학습\n",
    "\n",
    "  - <u>학습 데이터 입력 x와 $\\theta$에 따라 나온 예측값(predicted value)이 정답 y(label)와 비교하여 $\\theta$를 조절</u>해나가는 과정\n",
    "\n",
    "![](https://miro.medium.com/max/790/0*i6BmKLtTE5t89kJX)\n",
    "<br /><sub>출처: https://medium.com/@dhartidhami/machine-learning-basics-model-cost-function-and-gradient-descent-79b69ff28091</sub>\n",
    "\n",
    "  - 즉, **최적의 $\\theta$값에 따라 손실 함수의 가장 최저점(최솟값)을 찾는 과정**\n",
    "\n",
    "  - 손실 함수는 지도 학습(supervised learning) 알고리즘에서 반드시 필요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc70b7",
   "metadata": {},
   "source": [
    "### 원-핫 인코딩(one-hot encoding)\n",
    "- 범주형 변수를 표현할 때 사용\n",
    "- 가변수(Dummy Variable)이라고도 함\n",
    "- 정답인 레이블을 제외하고 0으로 처리  \n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*T5jaa2othYfXZX9W.)\n",
    "<sub>출처: https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179</sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dac2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5c662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffdc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccb57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a04c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed4bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ab442cd",
   "metadata": {},
   "source": [
    "### 평균절대오차(Mean Absolute Error, MAE)\n",
    "\n",
    "- 오차가 커져도 손실함수가 일정하게 증가\n",
    "\n",
    "- 이상치(Outlier)에 강건함(Robust)\n",
    "  -  데이터에서 [입력 - 정답] 관계가 적절하지 않은 것이 있을 경우에, 좋은 추정을 하더라도 오차가 발생하는 경우가 발생\n",
    "  - 그 때, 해당 이상치에 해당하는 지점에서 손실 함수의 최소값으로 가는 정도의 영향력이 크지 않음\n",
    "\n",
    "- 중간값(Median)과 연관\n",
    "\n",
    "- 회귀 (Regression)에 많이 사용\n",
    "\n",
    "![](https://miro.medium.com/max/1152/1*8BQhdKu1nk-tAAbOR17qGg.png)\n",
    "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>\n",
    "\n",
    "### $ \\qquad \\qquad E = \\frac{1}{n}\\sum_{i=1}^n \\left | y_i - \\tilde{y}_i \\right |$\n",
    "\n",
    "- $y_i$ : 학습 데이터의 $i\\ $번째 정답\n",
    "\n",
    "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e3b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf2f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac21860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52d1542d",
   "metadata": {},
   "source": [
    "### 평균제곱오차(Mean Squared Error, MSE)\n",
    "- 가장 많이 쓰이는 손실 함수 중 하나\n",
    "\n",
    "- 오차가 커질수록 손실함수가 빠르게 증가\n",
    "  - 정답과 예측한 값의 차이가 클수록 더 많은 페널티를 부여\n",
    "\n",
    "- 회귀 (Regression)에 쓰임\n",
    "\n",
    "![](https://miro.medium.com/max/1152/1*EqTaoCB1NmJnsRYEezSACA.png)\n",
    "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>\n",
    "\n",
    "### $ \\qquad \\qquad E = \\frac{1}{n}\\sum_{i=1}^n ( y_i - \\tilde{y}_i)^2 $\n",
    "\n",
    "  - $y_i$ : 학습 데이터의 $i\\ $번째 정답\n",
    "\n",
    "  - $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b4e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04b9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715caf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ce380b",
   "metadata": {},
   "source": [
    "### 손실함수로서의 MAE와 MSE 비교\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*JTC4ReFwSeAt3kvTLq1YoA.png)\n",
    "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef1f15",
   "metadata": {},
   "source": [
    "### 교차 엔트로피 오차(Cross Entropy Error, CEE)\n",
    "\n",
    "- 이진 분류(Binary Classification), 다중 클래스 분류(Multi Class Classification)\n",
    "\n",
    "- 소프트맥스(softmax)와 원-핫 인코딩(ont-hot encoding) 사이의 출력 간 거리를 비교\n",
    "\n",
    "- 정답인 클래스에 대해서만 오차를 계산\n",
    "  - 정답을 맞추면 오차가 0, 틀리면 그 차이가 클수록 오차가 무한히 커짐\n",
    "\n",
    "### $ \\qquad \\qquad E = - \\frac{1}{N}\\sum_{n} \\sum_{i}  y_i\\ log\\tilde{y}_i  $ \n",
    "\n",
    "- $y_i$ : 학습 데이터의 $i\\ $번째 정답 (원-핫 인코딩, one-hot encoding)\u001c",
    "\n",
    "\n",
    "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력\n",
    "\n",
    "- $N$ : 전체 데이터의 개수\n",
    "\n",
    "- $i$ : 데이터 하나당 클래스 개수\n",
    "\n",
    "- $y = log(x)$는\n",
    "  - $x$가 0에 가까울 수록 $y$값은 무한히 커짐\n",
    "  \n",
    "  - 1에 가까울 수록 0에 가까워짐\n",
    "   \n",
    "![](https://mathcracker.com/wp-content/uploads/2019/07/log-graph.png)\n",
    "<br /><sub>출처: https://mathcracker.com/finding-the-log-graph</sub>\n",
    "\n",
    "- 정답 레이블($y_i$)은 원-핫 인코딩으로 정답인 인덱스에만 1이고, 나머지는 모두 0\n",
    "\n",
    "- 따라서, 위 수식은 다음과 같이 나타낼 수 있음\n",
    "\n",
    "### $ \\qquad \\qquad E = - log\\tilde{y}_i  $\n",
    "\n",
    "  - 소프트맥스를 통해 나온 신경망 출력이 0.6이라면 $\\ -log0.6 \\fallingdotseq -0.51\\ $이 되고,  \n",
    "    신경망 출력이 0.3이라면  $\\ -log0.3 \\fallingdotseq -1.2\\ $이 됨\n",
    "\n",
    "  - 정답에 가까워질수록 오차값은 작아짐\n",
    "  \n",
    "  - 학습시, **원-핫 인코딩에 의해 정답 인덱스만 살아 남아 비교하지만, 정답이 아닌 인덱스들도 학습에 영향을 미침**  \n",
    "    <u>다중 클래스 분류는 소프트맥스(softmax) 함수를 통해 전체 항들을 모두 다루기 때문</u>\n",
    "\n",
    "![](https://miro.medium.com/max/836/1*T8KWtAn8FkAcsg8RsjiZ6Q.png)\n",
    "<br /><sub>출처: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9a857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38a81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a72f43a",
   "metadata": {},
   "source": [
    "#### 이진 분류에서의 교차 크로스 엔트로피(Binary Cross Entropy, BCE)\n",
    "\n",
    "- 이진 분류 문제(Binary Classification Problem)에서도 크로스 엔트로피 오차를 손실함수로 사용 가능\n",
    "\n",
    "### $ \\qquad \\qquad E = - \\sum_{i=1}^2  y_i\\ log\\tilde{y}_i \\\\ \n",
    "\\qquad \\qquad \\ \\ \\ = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)log(1-\\ \\tilde{y}_1) $  \n",
    "$\\qquad \\qquad \\qquad ( \\because y_2 = 1 - y_1)$\n",
    "\n",
    "- $y_i$ : 학습 데이터의 $i\\ $번째 정답 (원-핫 인코딩, one-hot encoding)\u001c",
    "\n",
    "\n",
    "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력\n",
    "\n",
    "\n",
    "- 2개의 클래스를 분류하는 문제에서  \n",
    "  1번이 정답일 확률이 0.8이고, 실제로 정답이 맞다면 위 식은 다음과 같이 나타낼 수 있음\n",
    "\n",
    "### $ \\qquad \\qquad E = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)\\ log\\ (1-\\ \\tilde{y}_1) \\\\\n",
    "\\qquad \\qquad \\ \\ \\ = -1\\ log\\ 0.8 - (1 - 1)\\ log\\ (1 - 0.8)\\\\ \n",
    "\\qquad \\qquad \\ \\ \\ = -log\\ 0.8 \\\\\n",
    "\\qquad \\qquad \\ \\ \\ \\fallingdotseq -0.22\n",
    "$\n",
    "\n",
    "- 반대로, 실제로 정답이 2번이었다면, 식은 다음과 같이 나타낼 수 있음\n",
    "\n",
    "### $ \\qquad \\qquad E = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)\\ log\\ (1-\\ \\tilde{y}_1) \\\\\n",
    "\\qquad \\qquad \\ \\ \\ = -0\\ log\\ 0.8 - (1 - 0)\\ log\\ (1 - 0.8)\\\\ \n",
    "\\qquad \\qquad \\ \\ \\ = -log\\ 0.2 \\\\\n",
    "\\qquad \\qquad \\ \\ \\ \\fallingdotseq -1.61\n",
    "$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14756f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794706e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
